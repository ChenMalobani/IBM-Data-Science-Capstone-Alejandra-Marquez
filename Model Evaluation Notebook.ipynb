{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    After training our model we perform our algorithm evaluation against test samples. Given that we are facing with an object detection problem, we are using two common metric called Average Precision(AP) and Average Recall(AR). We calculate these metrics using the Python COCO API. Coco is the well known large-scale object detection, segmentation, dataset, etc. The COCO API comes with tools for interacting with the COCO dataset, though we are only paying attention to the evaluation package.\n",
    "\n",
    "-    **The COCO API** is available on the following link: https://github.com/cocodataset/cocoapi \n",
    "\n",
    "\n",
    "-    We should download it and follow installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Once installed, we need two provide the toolkit with two JSON files. The first JSON file will contain all ground truth Annotations and the second JSON file will contain our predictions. The main difference between the contents of the JSON file and our XML Annotations is that the bounding box format is different. Whereas the XML format follows the (xmin,ymin,xmax,ymax) order, the JSON files follow the (x,y,w,h) format, where x,y represent the xmin, ymin coordinates, and w,h the width and height from the bounding box.\n",
    "\n",
    "\n",
    "-    We will perform the whole evaluation process in 4 stages:\n",
    "\n",
    "    1. The first step is to transform XML Annotations corresponding to the groundtruth and obtain the first JSON.\n",
    "            \n",
    "    2. In second step we will show how to transform the outputs from the inference stage using the Yolo algorithm.\n",
    "            \n",
    "    3. On the third step we will run the trained network to perform inference on the test samples and we will create the final JSON file using the function from the second step.\n",
    "            \n",
    "    4. Finally, using both JSONs we will evaluate prediction performance against the ground truth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transforming XML Annotations into JSON COCO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Our evaluation metrics will be obtained comparing the ground truth objects and confidence scores against our predictions. Since we are using the COCO API we need to transform our XML Annotations into a JSON format where the bounding box annotations follow the format: **(xmin,ymin,w,h)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     For this matter, we are going to use the **voc2coco** tool available on the following link:https://github.com/Tony607/voc2coco\n",
    "\n",
    "-    Once installed we should be able to generate our JSON file using the following command:\n",
    "\n",
    "            python voc2coco.py ./data/voc2012_raw/VOCdevkit/VOC2012/AnnotationsVal ./data/coco/output.json\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./validation_json.png\" width=\"600\" />\n",
    "\n",
    "The resulting JSON is showed on the image. Transformed bounding box coordinates are inside the 'bbox' tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Transforming YOLO detections into COCO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using COCO API, we need to transform the format of Yolo detections to the COCO format. The main difference is the bounding box format. While yolo outputs the bounding box in terms of xmin,ymin,xmax,ymax coordinates, COCO outputs are expressed on x,y,w,h coordinates\n",
    "\n",
    "We should transform outputs from all our images and create a JSON file that will serve as the input to the COCO API evaluation function.\n",
    "\n",
    "The transformation will be done by the **write_eval_file** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x1y1, x2y2):\n",
    "    x = x1y1[0]\n",
    "    y = x1y1[1]\n",
    "    w = x2y2[0] - x1y1[0]\n",
    "    h = x2y2[1] - x1y1[1]\n",
    "\n",
    "    return x,y,w,h\n",
    "\n",
    "def write_eval_file(img, image_name, outputs, class_names):\n",
    "    boxes, objectness, classes, nums = outputs\n",
    "    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]\n",
    "    wh = np.flip(img.shape[0:2])\n",
    "\n",
    "    image_id = image_name\n",
    "\n",
    "    tmp = list()\n",
    "    for i in range(nums):\n",
    "        x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.float64))\n",
    "        x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.float64))\n",
    "\n",
    "        x,y,w,h = xyxy2xywh(x1y1, x2y2)\n",
    "        \n",
    "        score = objectness[i].numpy().astype(np.float64)\n",
    "\n",
    "        tmp.append({\"image_id\":image_id,\n",
    "                    \"category_id\":int(classes[i]),\n",
    "                    \"bbox\":[round(x,2),round(y,2),round(w,2),round(h,2)],\n",
    "                    \"score\": score})\n",
    "        \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to read the filenames of the images for our evaluation. This images are located on the val.txt file. We read that file and store filenames under the **names_val** array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference using trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = 'checkpoints/yolov3_train_75.tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from absl import app, logging, flags\n",
    "from absl.flags import FLAGS\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from yolov3_tf2.models import (\n",
    "    YoloV3, YoloV3Tiny\n",
    ")\n",
    "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
    "from yolov3_tf2.utils import draw_outputs\n",
    "\n",
    "flags.DEFINE_string('classes', './data/coco.names', 'path to classes file')\n",
    "flags.DEFINE_string('weights', WEIGHTS,\n",
    "                    'path to weights file')\n",
    "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
    "flags.DEFINE_integer('size', 416, 'resize images to')\n",
    "flags.DEFINE_string('image', './data/girl.png', 'path to input image')\n",
    "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
    "flags.DEFINE_string('output', './output.jpg', 'path to output image')\n",
    "flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n",
    "\n",
    "flags.DEFINE_string('video','./data/2020-07-07-185827.mp4' ,'path to video file or number for webcam)')\n",
    "flags.DEFINE_string('output_format', 'XVID', 'codec used in VideoWriter when saving video to file')\n",
    "\n",
    "#LOAD WEIGHTS AND CREATE MODEL\n",
    "FLAGS.output = '/content/drive/My Drive/yolov3-tf2/data/temp.avi'\n",
    "FLAGS.num_classes = 3\n",
    "FLAGS.classes = 'data/voc2012.names'\n",
    "FLAGS.weights = WEIGHTS\n",
    "FLAGS.tiny = False\n",
    "\n",
    "# Lower threshold due to insufficient training\n",
    "FLAGS.yolo_iou_threshold = 0.2\n",
    "FLAGS.yolo_score_threshold = 0.2\n",
    "\n",
    "app._run_init(['yolov3'], app.parse_flags_with_usage)\n",
    "\n",
    "if FLAGS.tiny:\n",
    "    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n",
    "else:\n",
    "    yolo = YoloV3(classes=FLAGS.num_classes)\n",
    "\n",
    "yolo.load_weights(FLAGS.weights).expect_partial()\n",
    "logging.info('weights loaded')\n",
    "\n",
    "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
    "logging.info('classes loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read val.txt file to extract validation filenames for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val =  'data/voc2012_raw/VOCdevkit/VOC2012/ImageSets/Main/'\n",
    "val = open(path_val + 'val.txt',\"r\")\n",
    "Lines = val.readlines() \n",
    "names_val = []\n",
    "for line in Lines: \n",
    "    name = line.split()[0]\n",
    "    #name = name + '.png'\n",
    "    names_val.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through the samples, we perform inference using the previously trained weights. We then transform the inference outputs into the COCO JSON format using the function **write_eval_file** from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from yolov3_tf2.utils import draw_outputs\n",
    "\n",
    "import os\n",
    "\n",
    "image_path = '/content/drive/My Drive/yolov3-tf2/data/voc2012_raw/VOCdevkit/VOC2012/JPEGImages/'\n",
    "names = names_val\n",
    "eval_files = []\n",
    "\n",
    "for i, filename in enumerate(names):\n",
    "  if (i%50) == 0:\n",
    "    print(\"Image \",i)\n",
    "    print(image_path + FLAGS.image)\n",
    "\n",
    "  FLAGS.image = filename + '.png'\n",
    "  try:  \n",
    "    img_raw = tf.image.decode_image(\n",
    "        open(image_path + FLAGS.image, 'rb').read(), channels=3)\n",
    "  except:\n",
    "    try:\n",
    "      FLAGS.image = filename + '.jpg'\n",
    "      img_raw = tf.image.decode_image(\n",
    "        open(image_path + FLAGS.image, 'rb').read(), channels=3)\n",
    "    except:\n",
    "      try:\n",
    "        FLAGS.image = filename + '.jpeg'\n",
    "        img_raw = tf.image.decode_image(\n",
    "        open(image_path + FLAGS.image, 'rb').read(), channels=3)\n",
    "      except:\n",
    "        print(\"wrong image\")\n",
    "        continue\n",
    "\n",
    "  img = tf.expand_dims(img_raw, 0)\n",
    "  img = transform_images(img, FLAGS.size)\n",
    "\n",
    "  t1 = time.time()\n",
    "  #boxes, scores, classes, nums = yolo(img)\n",
    "  outputs = yolo(img)\n",
    "  t2 = time.time()\n",
    "  eval_file = write_eval_file(img, filename, outputs, class_names)\n",
    "  eval_files.append(eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of validation samples: \",len(eval_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump JSON predictions into a **results.json** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results.json', 'w',encoding='utf-8') as f:\n",
    "    json.dump(eval_files, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running demo for *bbox* results.\n"
     ]
    }
   ],
   "source": [
    "annType = ['segm','bbox','keypoints']\n",
    "annType = annType[1]      #specify type here\n",
    "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
    "print ('Running demo for *%s* results.'%(annType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the path containing the JSON file for the ground truth Annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "annFile = './valtiny.json'\n",
    "cocoGt=COCO(annFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the path containing the JSON file for the predicted Annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO detections api\n",
    "resFile = './valpredtiny.json'\n",
    "cocoDt=cocoGt.loadRes(resFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maksssksksss117']\n"
     ]
    }
   ],
   "source": [
    "imgIds=cocoGt.getImgIds()\n",
    "imgIds=imgIds[0:100]\n",
    "print(imgIds)\n",
    "#imgId = imgIds[np.random.randint(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
